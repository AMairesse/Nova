# Define the database username and password
DB_USER=postgres
DB_PASSWORD=secret      # Change to strong password!

# Superuser auto-creation (optional; for first run)
DJANGO_SUPERUSER_USERNAME=admin
DJANGO_SUPERUSER_EMAIL=admin@example.com
DJANGO_SUPERUSER_PASSWORD=changeme  # Change to strong password!

# Define Minio admin credentials
MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=changeme  # Change to strong password!

# Define encryption key
FIELD_ENCRYPTION_KEY='BgZFdTx6-RVz9_EwMjrazWJevn9-ArSf4z6EvcUx2q8=' # Change to strong key!
# You can generate a new key with this Python code :
# from cryptography.fernet import Fernet
# key = Fernet.generate_key()
# print(key.decode())

# Define the Django settings
DJANGO_SECRET_KEY='django-insecure-aah#w*cjs_a6(3@-)qh!023ssib6(^4-gw%so_wwizh2n_1*th' # Change to strong key!
# You can generate a new key with this Python code :
# from django.core.management.utils import get_random_secret_key
# print(get_random_secret_key())

# Define hosting parameters
# HOST_PORT allow only one port
HOST_PORT=8080
# ALLOWED_HOSTS allow multiple hostnames or IP separated by comma
ALLOWED_HOSTS=localhost
# CSRF_TRUSTED_ORIGINS allow multiple http or https URLs separated by comma
CSRF_TRUSTED_ORIGINS=http://localhost:$HOST_PORT

# MinIO for Nova (file storage)
MINIO_BUCKET_NAME=nova-files  # Bucket name for Nova files (change if needed)
MINIO_ACCESS_KEY=nova_user    # Access key for Nova's MinIO user (change if needed)
MINIO_SECRET_KEY=your_strong_secret_here  # Secret key for Nova (REQUIRED: generate a strong one, e.g., openssl rand -hex 20)

# Optional: define specific Ollama settings
# WARNING : your should uncomment only if you add the ollama docker compose file
# E.g. : docker compose -f docker-compose.yml -f docker-compose.add-ollama.yml up -d
# OLLAMA_MODEL_NAME=NovaModel
# OLLAMA_CONTEXT_LENGTH=4096

# Optional: define specific llama.cpp settings
# WARNING : your should uncomment only if you add the ollama docker compose file
# E.g. : docker compose -f docker-compose.yml -f docker-compose.add-llamacpp.yml up -d
# LLAMA_CPP_MODEL=qwen/qwen3-8B-GGUF    # Use huggingface model's name, it will be downloaded
# LLAMA_CPP_CHAT_TEMPLATE=chatml        # Use the recommanded chat template for your model (default: chatml)
# LLAMA_CPP_CTX_SIZE=4096
# LLAMA_CPP_THINKING_BUDGET=-1          # Use -1 to allow thinking, use 0 to block thinking (on relevant models)

# Optional: activate debug mode
# DEBUG=True
